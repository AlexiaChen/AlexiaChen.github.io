<html>
<head><meta name="generator" content="Hexo 3.8.0">
	
	<title>防止数据被恢复的可能</title>
	<meta name="keywords" content="fzb.me,冯宗宝,冯宗宝的blog">

    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    
	   <link href="/blog/css/main.css?v=3" rel="stylesheet" type="text/css">
    
        <script src="/blog/js/util.js"></script>
        <script>
            if(isMobile()) {
                loadjscssfile('../css/mobile.css', 'css');
            } else {
                loadjscssfile('../css/desktop.css', 'css');
            }
        </script> 
    

    <link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed">

    
	<link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico?v=3">
    
    

</head>

<body>


<h2 class="title">防止数据被恢复的可能</h2>
<!---
<div style="text-align:center;margin-top: -10px;">
<div class="article-category">
发表于2016年9月2日




 </div>
--->



<p> 平时大家有没有好奇过，为什么硬盘上被删除的数据有时候还能被还原恢复呢？其中的原理是什么呢？</p>
<p> 其实，如果操作系统学得不错的人应该知道，数据文件被删除只是操作系统把文件从文件系统中删除，也就是文件在文件系统中的索引（地址）被删除，意思就是，操作系统告诉文件系统：Hey，哥们儿，我不需要这个文件了，你把这文件从检索索引中删除吧，我不再需要了。其实真实的文件数据还会真实存在物理硬盘上，没有真正被消除，除非有新的文件数据被保存并恰好覆盖了之前的文件数据。数据恢复就是依赖这个原理工作的。</p>
<p> 那么如何防止数据被恢复的可能呢？其实如果脑袋聪明的人一下子就能想到，我往原文件里面大量反复写入Dirty Data来覆盖之前的数据不就可以了么？答对，就是这个思路。其实现今无论多么先进的数据销毁软件都是基于这么个思路（甚至美国中央情报局销毁绝密数据也是这么干的）。只不过越要降低被恢复的可能性写入的次数随之增加，所以耗费的时间也越多。</p>
<p> 垃圾数据只写入一次还不够，需要多次覆盖，这是因为，直观上理解，一次写入后，磁盘上的数据就变化了。其实在硬件层面，不是这样的，还是能够恢复的。恢复的技术有下面两种：</p>
<ul>
<li>第一种是：当硬盘读写头在硬盘上写入一位数据时，它使用的信号强度只是用来写入一位数据，并不会影响到相邻位的数据。由于这个写入信号并不是很强，因此你可以通过它写入的数据位的绝对信号强度来判断此前该数据位所保存的是何种数据。换句话说，如果二进制数据0被二进制数据1所覆盖，其信号强度会比二进制数据1被覆盖要弱一些。使用专门的硬件就可以检测出准确的信号强度。把被覆盖区域读出的信号减去所覆盖数据的标准信号强度，就能获得原先数据的一个副本。更令人吃惊的是，这一恢复过程可以被重复7次。因此如果想避免别人使用这种方法来窃取你的数据，至少要覆盖该数据区域7次，而且每次还应该使用不同的随机数据。</li>
</ul>
<ul>
<li>第二种数据恢复技术则是利用了硬盘读写头的另一个特点：读写头每次进行写操作的位置并不一定对得十分精确。这就能让专家们在磁道的边缘侦测到原有的数据（也被称为影子数据）。只有重复地覆写数据才能消除这些磁道边缘的影子数据。由于硬件的这种特性，在销毁文件的时候，才会需要多次覆盖。</li>
</ul>
<p>所以根据以上的特性，才有了许多数据擦除算法的诞生，覆盖次数越多，随机垃圾数据越“随机”，被恢复的可能性也就越低，安全性也就越高。到了<a href="https://en.wikipedia.org/wiki/Gutmann_method" target="_blank" rel="noopener">Gutman method</a>这样的级别，理论上已经证明了，经过此算法处理过的数据，没可能恢复。 但是，弱点就是速度很慢，如果需要有大量机密数据要被销毁，这将是耗时的任务。</p>
<p>我写过一个<a href="https://github.com/AlexiaChen/DataBlackHole" target="_blank" rel="noopener">数据擦除的库</a>，等级最高也就是支持到Gutman这个级别。其中<a href="https://ia.signal.army.mil/docs/DOD5220_22M/522022m.htm" target="_blank" rel="noopener">DOD5220_22M</a>算法是之前美国中央情报局采用的，不过现在被抛弃了，我猜测原因就是速度和安全性的权衡。至于他们为什么没有采用Gutman算法，其实也是速度和安全性的权衡，因为太慢了:) 这告诉我们，评价衡量一个工具需要综合考量。</p>


<!--<a href="https://alexiachen.github.io/blog/2016/09/02/data-erasure/#disqus_thread" class="article-comment-link">Comments</a>
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = ''; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
-->
<div style="display:none">
<script src="http://s4.cnzz.com/stat.php?id=&web_id=" language="JavaScript"></script>script>
</div>






</body>
</html>